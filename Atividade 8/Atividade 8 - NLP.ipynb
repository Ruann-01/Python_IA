{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução a NLP\n",
    "Nessa atividade vocês irão trabalhar em um problema de classificação de texto multiclasse. Considere o conjunto de dados sobre fetch_20newsgroups  \n",
    "\n",
    "\"The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. To the best of our knowledge, it was originally collected by Ken Lang, probably for his paper “Newsweeder: Learning to filter netnews,” though he does not explicitly mention this collection. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering.\"\n",
    "\n",
    "Esse conjunto de dados pode ser carregado através so scikit-learn\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "twenty_train = fetch_20newsgroups(subset='train',\n",
    "                                  shuffle=True, random_state=42)\n",
    "\n",
    "twenty_test = fetch_20newsgroups(subset='test', \n",
    "                                 shuffle=True, random_state=42)\n",
    "\n",
    "Dado esse contexto, escolha um único classificador, sem otimizar hiperparametros, treine e teste modelos considerando\n",
    "- Bag of Words (contagem), sem pré-processamento\n",
    "- TF-IDF, sem pré-processamento \n",
    "- Bag of Words, com pré-processamento\n",
    "- TF-IDF, com pré-processamento\n",
    "- Considere a métrica da acurácia e compare os resultados em uma tabela.\n",
    "\n",
    "As etapas de pré-processamento devem conter pelo menos:\n",
    "- lowercase\n",
    "- remoção de pontuação\n",
    "- remoção de números \n",
    "- remoção de stopwords (dica: utilize a biblioteca NLTK)\n",
    "- lematização ou stemming (apenas um dos dois)\n",
    "\n",
    "Outras etapas que você julgar necessárias podem ser utilizadas. Crie uma função para cada etapa e uma função chamada preprocess() que chame todas as etapas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ruann\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ruann\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ruann\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True, random_state=42)\n",
    "\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train = twenty_train['data']\n",
    "y_train = twenty_train['target']\n",
    "text_test = twenty_test['data']\n",
    "y_test = twenty_test['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fazendo extração\n",
    "Dado esse contexto, escolha um único classificador, sem otimizar hiperparametros, treine e teste modelos considerando\n",
    "- Bag of Words (contagem), sem pré-processamento\n",
    "- TF-IDF, sem pré-processamento "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "bagwords = CountVectorizer()\n",
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_b = bagwords.fit_transform(text_train)\n",
    "X_text_b = bagwords.transform(text_test)\n",
    "X_train_tf = tfidf.fit_transform(text_train)\n",
    "X_text_tf = tfidf.transform(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_b;\n",
    "X_train_tf;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classificador Escolhido: Regressão Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train_b, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = LogisticRegression()\n",
    "model2.fit(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A acurácia do bagwords, foi: 78.93\n",
      "A acurácia do tf-idf, foi: 82.74\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "y_pred = model.predict(X_text_b)\n",
    "y_pred2 = model2.predict(X_text_tf)\n",
    "acuracia1 = round(accuracy_score(y_test,y_pred)*100,2)\n",
    "acuracia2 = round(accuracy_score(y_test,y_pred2)*100,2)\n",
    "print('A acurácia do bagwords, foi:' ,acuracia1)\n",
    "print('A acurácia do tf-idf, foi:' ,acuracia2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré-Processamento \n",
    "\n",
    "As etapas de pré-processamento devem conter pelo menos:\n",
    "- lowercase\n",
    "- remoção de pontuação\n",
    "- remoção de números \n",
    "- remoção de stopwords (dica: utilize a biblioteca NLTK)\n",
    "- lematização ou stemming (apenas um dos dois)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowering(texto):\n",
    "    return texto.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def remocao_pontuacao(texto):\n",
    "    pontuacao_retirada = \"\".join([i for i in texto if i not in string.punctuation])\n",
    "    return pontuacao_retirada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remocao_numeros(texto):\n",
    "    number_regex = '\\d+'\n",
    "    x = re.sub(number_regex, '', texto)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remocao_stopwords(texto):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    texto_stopwords = [j for j in texto.split() if j not in stopwords]\n",
    "    frase = \" \".join(texto_stopwords)\n",
    "    return frase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "def lemmatizacao(texto):\n",
    "    lemm_texto = [wordnet_lemmatizer.lemmatize(word) for word in texto.split()]\n",
    "    frase = \" \".join(lemm_texto)\n",
    "    return frase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessamento(texto):\n",
    "    LO = lowering(texto)\n",
    "    RP = remocao_pontuacao(LO)\n",
    "    RN = remocao_numeros(RP)\n",
    "    RS = remocao_stopwords(RN)\n",
    "    LE = lemmatizacao(RS)\n",
    "    return LE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_p = []\n",
    "for i in range(len(text_train)):\n",
    "    p_process = preprocessamento(text_train[i])\n",
    "    X_train_p.append(p_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_p = []\n",
    "for i in range(len(text_test)):\n",
    "    p_process = preprocessamento(text_test[i])\n",
    "    X_test_p.append(p_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train[0];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_p[0];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_test[0];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_p[0];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fazendo extração\n",
    "Dado esse contexto, escolha um único classificador, sem otimizar hiperparametros, treine e teste modelos considerando\n",
    "- Bag of Words, com pré-processamento\n",
    "- TF-IDF, com pré-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pb = bagwords.fit_transform(X_train_p)\n",
    "X_text_pb = bagwords.transform(X_test_p)\n",
    "X_train_ptf = tfidf.fit_transform(X_train_p)\n",
    "X_text_ptf = tfidf.transform(X_test_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classificador Escolhido: Regressão Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3 = LogisticRegression(max_iter=1000)\n",
    "model3.fit(X_train_pb, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4 = LogisticRegression()\n",
    "model4.fit(X_train_ptf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A acurácia do bagwords após pre-processamento, foi: 79.63\n",
      "A acurácia do tf-idf após pre-processamento, foi: 83.23\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "y_pred3 = model3.predict(X_text_pb)\n",
    "y_pred4 = model4.predict(X_text_ptf)\n",
    "acuracia3 = round(accuracy_score(y_test,y_pred3)*100,2)\n",
    "acuracia4 = round(accuracy_score(y_test,y_pred4)*100,2)\n",
    "print('A acurácia do bagwords após pre-processamento, foi:' ,acuracia3)\n",
    "print('A acurácia do tf-idf após pre-processamento, foi:' ,acuracia4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparação de Resultados - Tabela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

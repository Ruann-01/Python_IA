{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução a NLP\n",
    "Nessa atividade vocês irão trabalhar em um problema de classificação de texto multiclasse. Considere o conjunto de dados sobre fetch_20newsgroups  \n",
    "\n",
    "\"The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. To the best of our knowledge, it was originally collected by Ken Lang, probably for his paper “Newsweeder: Learning to filter netnews,” though he does not explicitly mention this collection. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering.\"\n",
    "\n",
    "Esse conjunto de dados pode ser carregado através so scikit-learn\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "twenty_train = fetch_20newsgroups(subset='train',\n",
    "                                  shuffle=True, random_state=42)\n",
    "\n",
    "twenty_test = fetch_20newsgroups(subset='test', \n",
    "                                 shuffle=True, random_state=42)\n",
    "\n",
    "Dado esse contexto, escolha um único classificador, sem otimizar hiperparametros, treine e teste modelos considerando\n",
    "- Bag of Words (contagem), sem pré-processamento\n",
    "- TF-IDF, sem pré-processamento \n",
    "- Bag of Words, com pré-processamento\n",
    "- TF-IDF, com pré-processamento\n",
    "- Considere a métrica da acurácia e compare os resultados em uma tabela.\n",
    "\n",
    "As etapas de pré-processamento devem conter pelo menos:\n",
    "- lowercase\n",
    "- remoção de pontuação\n",
    "- remoção de números \n",
    "- remoção de stopwords (dica: utilize a biblioteca NLTK)\n",
    "- lematização ou stemming (apenas um dos dois)\n",
    "\n",
    "Outras etapas que você julgar necessárias podem ser utilizadas. Crie uma função para cada etapa e uma função chamada preprocess() que chame todas as etapas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True, random_state=42)\n",
    "\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train = twenty_train['data']\n",
    "y_train = twenty_train['target']\n",
    "text_test = twenty_test['data']\n",
    "y_test = twenty_test['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fazendo extração\n",
    "Dado esse contexto, escolha um único classificador, sem otimizar hiperparametros, treine e teste modelos considerando\n",
    "- Bag of Words (contagem), sem pré-processamento\n",
    "- TF-IDF, sem pré-processamento "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "bagwords = CountVectorizer()\n",
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_b = bagwords.fit_transform(text_train)\n",
    "X_text_b = bagwords.transform(text_test)\n",
    "X_train_tf = tfidf.fit_transform(text_train)\n",
    "X_text_tf = tfidf.transform(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_b;\n",
    "X_train_tf;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classificador Escolhido: Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train_b, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model2 = LogisticRegression()\n",
    "model2.fit(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A acurácia do bagwords, foi: 78.92989909718534\n",
      "A acurácia do tf-idf, foi: 82.74030801911843\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "y_pred = model.predict(X_text_b)\n",
    "y_pred2 = model2.predict(X_text_tf)\n",
    "acuracia1 = accuracy_score(y_test,y_pred)*100\n",
    "acuracia2 = accuracy_score(y_test,y_pred2)*100\n",
    "print('A acurácia do bagwords, foi:' ,acuracia1)\n",
    "print('A acurácia do tf-idf, foi:' ,acuracia2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fazendo Extração\n",
    "Dado esse contexto, escolha um único classificador, sem otimizar hiperparametros, treine e teste modelos considerando\n",
    "- Bag of Words, com pré-processamento\n",
    "- TF-IDF, com pré-processamento\n",
    "- Considere a métrica da acurácia e compare os resultados em uma tabela.\n",
    "\n",
    "As etapas de pré-processamento devem conter pelo menos:\n",
    "- lowercase\n",
    "- remoção de pontuação\n",
    "- remoção de números \n",
    "- remoção de stopwords (dica: utilize a biblioteca NLTK)\n",
    "- lematização ou stemming (apenas um dos dois)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
